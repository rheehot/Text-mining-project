{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "#html 태그 제거\n",
    "import re\n",
    "#> 제거\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Hannanum, Kkma, Komoran, Okt\n",
    "\n",
    "#파일명 추출\n",
    "data_zip = zipfile.ZipFile('data (2).zip')\n",
    "dataList = data_zip.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['statement0.txt',\n",
       " 'statement1.txt',\n",
       " 'statement10.txt',\n",
       " 'statement101.txt',\n",
       " 'statement102.txt',\n",
       " 'statement103.txt',\n",
       " 'statement104.txt',\n",
       " 'statement105.txt',\n",
       " 'statement106.txt',\n",
       " 'statement107.txt',\n",
       " 'statement108.txt',\n",
       " 'statement109.txt',\n",
       " 'statement11.txt',\n",
       " 'statement110.txt',\n",
       " 'statement111.txt',\n",
       " 'statement112.txt',\n",
       " 'statement113.txt',\n",
       " 'statement114.txt',\n",
       " 'statement115.txt',\n",
       " 'statement116.txt',\n",
       " 'statement117.txt',\n",
       " 'statement118.txt',\n",
       " 'statement119.txt',\n",
       " 'statement12.txt',\n",
       " 'statement120.txt',\n",
       " 'statement121.txt',\n",
       " 'statement122.txt',\n",
       " 'statement123.txt',\n",
       " 'statement124.txt',\n",
       " 'statement125.txt',\n",
       " 'statement126.txt',\n",
       " 'statement127.txt',\n",
       " 'statement128.txt',\n",
       " 'statement129.txt',\n",
       " 'statement13.txt',\n",
       " 'statement130.txt',\n",
       " 'statement131.txt',\n",
       " 'statement132.txt',\n",
       " 'statement133.txt',\n",
       " 'statement134.txt',\n",
       " 'statement135.txt',\n",
       " 'statement136.txt',\n",
       " 'statement137.txt',\n",
       " 'statement138.txt',\n",
       " 'statement139.txt',\n",
       " 'statement14.txt',\n",
       " 'statement140.txt',\n",
       " 'statement141.txt',\n",
       " 'statement142.txt',\n",
       " 'statement143.txt',\n",
       " 'statement144.txt',\n",
       " 'statement145.txt',\n",
       " 'statement146.txt',\n",
       " 'statement147.txt',\n",
       " 'statement148.txt',\n",
       " 'statement149.txt',\n",
       " 'statement15.txt',\n",
       " 'statement150.txt',\n",
       " 'statement151.txt',\n",
       " 'statement16.txt',\n",
       " 'statement17.txt',\n",
       " 'statement18.txt',\n",
       " 'statement19.txt',\n",
       " 'statement2.txt',\n",
       " 'statement20.txt',\n",
       " 'statement21.txt',\n",
       " 'statement22.txt',\n",
       " 'statement23.txt',\n",
       " 'statement24.txt',\n",
       " 'statement25.txt',\n",
       " 'statement26.txt',\n",
       " 'statement27.txt',\n",
       " 'statement28.txt',\n",
       " 'statement29.txt',\n",
       " 'statement3.txt',\n",
       " 'statement30.txt',\n",
       " 'statement31.txt',\n",
       " 'statement32.txt',\n",
       " 'statement33.txt',\n",
       " 'statement34.txt',\n",
       " 'statement35.txt',\n",
       " 'statement36.txt',\n",
       " 'statement37.txt',\n",
       " 'statement38.txt',\n",
       " 'statement39.txt',\n",
       " 'statement4.txt',\n",
       " 'statement40.txt',\n",
       " 'statement41.txt',\n",
       " 'statement42.txt',\n",
       " 'statement43.txt',\n",
       " 'statement44.txt',\n",
       " 'statement45.txt',\n",
       " 'statement46.txt',\n",
       " 'statement47.txt',\n",
       " 'statement48.txt',\n",
       " 'statement49.txt',\n",
       " 'statement5.txt',\n",
       " 'statement50.txt',\n",
       " 'statement51.txt',\n",
       " 'statement52.txt',\n",
       " 'statement53.txt',\n",
       " 'statement54.txt',\n",
       " 'statement55.txt',\n",
       " 'statement56.txt',\n",
       " 'statement57.txt',\n",
       " 'statement58.txt',\n",
       " 'statement59.txt',\n",
       " 'statement6.txt',\n",
       " 'statement60.txt',\n",
       " 'statement61.txt',\n",
       " 'statement62.txt',\n",
       " 'statement63.txt',\n",
       " 'statement64.txt',\n",
       " 'statement65.txt',\n",
       " 'statement66.txt',\n",
       " 'statement67.txt',\n",
       " 'statement68.txt',\n",
       " 'statement69.txt',\n",
       " 'statement7.txt',\n",
       " 'statement70.txt',\n",
       " 'statement71.txt',\n",
       " 'statement72.txt',\n",
       " 'statement73.txt',\n",
       " 'statement74.txt',\n",
       " 'statement75.txt',\n",
       " 'statement76.txt',\n",
       " 'statement77.txt',\n",
       " 'statement78.txt',\n",
       " 'statement79.txt',\n",
       " 'statement8.txt',\n",
       " 'statement80.txt',\n",
       " 'statement81.txt',\n",
       " 'statement82.txt',\n",
       " 'statement83.txt',\n",
       " 'statement84.txt',\n",
       " 'statement85.txt',\n",
       " 'statement86.txt',\n",
       " 'statement87.txt',\n",
       " 'statement88.txt',\n",
       " 'statement89.txt',\n",
       " 'statement9.txt',\n",
       " 'statement90.txt',\n",
       " 'statement91.txt',\n",
       " 'statement92.txt',\n",
       " 'statement93.txt',\n",
       " 'statement94.txt',\n",
       " 'statement95.txt',\n",
       " 'statement96.txt',\n",
       " 'statement97.txt',\n",
       " 'statement98.txt',\n",
       " 'statement99.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오는 함수 만들기\n",
    "def tokenizer(list):\n",
    "    \n",
    "    textFile = []\n",
    "    for i in range(len(list)):\n",
    "        \n",
    "        f = open('data (2)/statement{}.txt'.format(i),'r',encoding='utf-8')\n",
    "        line = f.readlines()\n",
    "        \n",
    "        \n",
    "        #태그 제거\n",
    "        textList = []\n",
    "        pattern = '<[^>]*>|\\n'\n",
    "    \n",
    "        line = re.sub(pattern=pattern, repl='', string=line[0])\n",
    "        line = BeautifulSoup(line, 'html.parser').get_text()\n",
    "        line = line.replace(\"[\", \"\").replace(\"<\", \"\").replace(\">\",\"\").replace(\"]\", \"\")\n",
    "        \n",
    "        textList.append(line)\n",
    "        #print(textList)\n",
    "        \n",
    "        #처리한 데이터를 textFile 리스트에 저장\n",
    "        textFile.append(textList)\n",
    "        print(textFile)\n",
    "    \n",
    "    return textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#komoran을 사용하여 형태소에 pos태그를 붙이기\n",
    "def komoran_pos(tokens_list):\n",
    "    komoran = Komoran()\n",
    "    kom_pos = []\n",
    "    \n",
    "    for i in tokens_list:\n",
    "        #print(i[0])\n",
    "        x = komoran.pos(i[0])\n",
    "        #print(x)\n",
    "        kom_pos.append(x)\n",
    "    return kom_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kom_pos = komoran_pos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "\n",
    "class KeysentenceSummarizer:\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        Tokenize function: tokenize(str) = list of str\n",
    "    min_count : int\n",
    "        Minumum frequency of words will be used to construct sentence graph\n",
    "    min_sim : float\n",
    "        Minimum similarity between sentences in sentence graph\n",
    "    similarity : str\n",
    "        available similarity = ['cosine', 'textrank']\n",
    "    vocab_to_idx : dict or None\n",
    "        Vocabulary to index mapper\n",
    "    df : float\n",
    "        PageRank damping factor\n",
    "    max_iter : int\n",
    "        Number of PageRank iterations\n",
    "    verbose : Boolean\n",
    "        If True, it shows training progress\n",
    "    \"\"\"\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.min_sim = min_sim\n",
    "        self.similarity = similarity\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "            Shape must be (n_sents,)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
    "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def summarize(self, sents, topk=30, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        topk : int\n",
    "            Number of key-sentences to be selected.\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "            Shape must be (n_sents,)\n",
    "        Returns\n",
    "        -------\n",
    "        keysents : list of tuple\n",
    "            Each tuple stands for (sentence index, rank, sentence)\n",
    "        Usage\n",
    "        -----\n",
    "            >>> from textrank import KeysentenceSummarizer\n",
    "            >>> summarizer = KeysentenceSummarizer(tokenize = tokenizer, min_sim = 0.5)\n",
    "            >>> keysents = summarizer.summarize(texts, topk=30)\n",
    "        \"\"\"\n",
    "        n_sents = len(sents)\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            if bias.shape != (n_sents,):\n",
    "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
    "        elif bias is not None:\n",
    "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
    "\n",
    "        self.train_textrank(sents, bias)\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "        return keysents\n",
    "    \n",
    "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
    "    similarity=None, vocab_to_idx=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(sent) return list of str\n",
    "    min_count : int\n",
    "        Minimum term frequency\n",
    "    min_sim : float\n",
    "        Minimum similarity between sentences\n",
    "    similarity : callable or str\n",
    "        similarity(s1, s2) returns float\n",
    "        s1 and s2 are list of str.\n",
    "        available similarity = [callable, 'cosine', 'textrank']\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "        If None, this function scan vocabulary first.\n",
    "    verbose : Boolean\n",
    "        If True, verbose mode on\n",
    "    Returns\n",
    "    -------\n",
    "    sentence similarity graph : scipy.sparse.csr_matrix\n",
    "        shape = (n sents, n sents)\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_to_idx is None:\n",
    "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    else:\n",
    "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
    "\n",
    "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
    "    if similarity == 'cosine':\n",
    "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    else:\n",
    "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    return x\n",
    "\n",
    "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, sent in enumerate(sents):\n",
    "        counter = Counter(tokenize(sent))\n",
    "        for token, count in counter.items():\n",
    "            j = vocab_to_idx.get(token, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(count)\n",
    "    n_rows = len(sents)\n",
    "    n_cols = len(vocab_to_idx)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # Boolean matrix\n",
    "    rows, cols = x.nonzero()\n",
    "    data = np.ones(rows.shape[0])\n",
    "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "    # Inverse sentence length\n",
    "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
    "    size[np.where(size <= min_length)] = 10000\n",
    "    size = np.log(size)\n",
    "\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "\n",
    "        # slicing\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx+1) * batch_size))\n",
    "\n",
    "        # dot product\n",
    "        inner = z[b:e,:] * z.transpose()\n",
    "\n",
    "        # sentence len[i,j] = size[i] + size[j]\n",
    "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
    "        norm = norm ** (-1)\n",
    "        norm[np.where(norm == np.inf)] = 0\n",
    "\n",
    "        # normalize\n",
    "        sim = inner.multiply(norm).tocsr()\n",
    "        rows, cols = (sim >= min_sim).nonzero()\n",
    "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
    "\n",
    "        # append\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
    "\n",
    "        if verbose:\n",
    "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
    "\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(str) returns list of str\n",
    "    min_count : int\n",
    "        Minumum term frequency\n",
    "    Returns\n",
    "    -------\n",
    "    idx_to_vocab : list of str\n",
    "        Vocabulary list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "    \"\"\"\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : scipy.sparse.csr_matrix\n",
    "        shape = (n vertex, n vertex)\n",
    "    df : float\n",
    "        Damping factor, 0 < df < 1\n",
    "    max_iter : int\n",
    "        Maximum number of iteration\n",
    "    bias : numpy.ndarray or None\n",
    "        If None, equal bias\n",
    "    Returns\n",
    "    -------\n",
    "    R : numpy.ndarray\n",
    "        PageRank vector. shape = (n vertex, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # check bias\n",
    "    if bias is None:\n",
    "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "    else:\n",
    "        bias = bias.reshape(-1,1)\n",
    "        bias = A.shape[0] * bias / bias.sum()\n",
    "        assert bias.shape[0] == A.shape[0]\n",
    "        bias = (1 - df) * bias\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['신세돈 교수님, 지역화폐 비판 주장의 논거를 마저 듣고 싶습니다MBC 백분토론에서 신세돈 교수님을 만나 의견 견줄 수 있어 감사했습니다. 그런데 신 교수님 께서 ‘지역화폐가 1년 지나면 반드시 실패할 것이다. 부정유통 위험이 있다’고 주장하시곤 시간이 없어서였는지 이렇다할 논거를 말씀하시지 못했습니다.그러다보니 신 교수님의 주장은 시중에 나도는 어처구니 없는 주장인 ‘경기도가 지역화폐로 깡을 하게 한다’느니 ‘중간 업자가 끼어서 부정 소지가 있다’느니 하는 것들과 다를바 없이 들렸습니다.경기도내 지역화폐는 지류(상품권)와 카드나 앱 등 전자화폐가 있고 어느 것을 사용할 지는 시군이 선택합니다만, 거의 대부분 전자화폐로 현금카드와 동일하게 사용됩니다.지류(상품권)는 조폐공사에 제작비를 주고 인쇄하며 이를 받은 중소상공인은 전액 현금으로 은행에서 환전할 수 있기 때문에 소위 말하는 깡(할인 매매)은 생각하기 어렵습니다.전자화폐역시 전산시스템 운영자에게 소액의 약정된 수수료를 지급하고 사용자는 단 한푼의 손실도 없이 전액을 체크카드처럼 사용하기 때문에 중간 사업자의 비리란 있을 수 없습니다.지역화폐는 이미 5년 이상 성남시와 경기도에서는 물론 전국으로 확산되어 아무 부작용이나 비리 없이 투명하게 잘 사용되고 있습니다.박근혜 전 대통령의 경제교사이고 UCLA 경제학 박사이시며 제 1야당의 경제정책을 총괄하는 신 교수님께서 악성 허위 댓글과 보수경제지의 악의적 보도 때문에 객관적 인식과 판단이 왜곡되신 것은 아닐 거라 믿습니다.토론에서의 ‘비리 연루 가능성’ 주장이 잘못임을 인정하시면 발언을 수정해 주시고, 여전히 동일한 주장을 하시면 그 논거를 마저 듣고 싶으니 공개토론에 한 번 더 응해주시면 좋겠습니다.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['신세돈 교수님, 지역화폐 비판 주장의 논거를 마저 듣고 싶습니다',\n",
    "        'MBC 백분토론에서 신세돈 교수님을 만나 의견 견줄 수 있어 감사했습니다',\n",
    "        '그런데 신 교수님 께서 ‘지역화폐가 1년 지나면 반드시 실패할 것이다.',\n",
    "        '부정유통 위험이 있다’고 주장하시곤 시간이 없어서였는지 이렇다할 논거를 말씀하시지 못했습니다',\n",
    "        '그러다보니 신 교수님의 주장은 시중에 나도는 어처구니 없는 주장인 ‘경기도가 지역화폐로 깡을 하게 한다’느니 ‘중간 업자가 끼어서 부정 소지가 있다’느니 하는 것들과 다를바 없이 들렸습니다.',\n",
    "        '경기도내 지역화폐는 지류(상품권)와 카드나 앱 등 전자화폐가 있고 어느 것을 사용할 지는 시군이 선택합니다만',\n",
    "        '거의 대부분 전자화폐로 현금카드와 동일하게 사용됩니다.',\n",
    "        '지류(상품권)는 조폐공사에 제작비를 주고 인쇄하며 이를 받은 중소상공인은 전액 현금으로 은행에서 환전할 수 있기 때문에 소위 말하는 깡(할인 매매)은 생각하기 어렵습니다.',\n",
    "        '전자화폐역시 전산시스템 운영자에게 소액의 약정된 수수료를 지급하고 사용자는 단 한푼의 손실도 없이 전액을 체크카드처럼 사용하기 때문에 중간 사업자의 비리란 있을 수 없습니다.',\n",
    "        '지역화폐는 이미 5년 이상 성남시와 경기도에서는 물론 전국으로 확산되어 아무 부작용이나 비리 없이 투명하게 잘 사용되고 있습니다.',\n",
    "        '전 대통령의 경제교사이고 UCLA 경제학 박사이시며 제 1야당의 경제정책을 총괄하는 신 교수님께서 악성 허위 댓글과 보수경제지의 악의적 보도 때문에 객관적 인식과 판단이 왜곡되신 것은 아닐 거라 믿습니다.',\n",
    "        '박근혜 토론에서의 ‘비리 연루 가능성’ 주장이 잘못임을 인정하시면 발언을 수정해 주시고, 여전히 동일한 주장을 하시면 그 논거를 마저 듣고 싶으니 공개토론에 한 번 더 응해주시면 좋겠습니다',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전자화폐역시 전산시스템 운영자에게 소액의 약정된 수수료를 지급하고 사용자는 단 한푼의 손실도 없이 전액을 체크카드처럼 사용하기 때문에 중간 사업자의 비리란 있을 수 없습니다.\n",
      "지류(상품권)는 조폐공사에 제작비를 주고 인쇄하며 이를 받은 중소상공인은 전액 현금으로 은행에서 환전할 수 있기 때문에 소위 말하는 깡(할인 매매)은 생각하기 어렵습니다.\n",
      "신세돈 교수님, 지역화폐 비판 주장의 논거를 마저 듣고 싶습니다\n"
     ]
    }
   ],
   "source": [
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = lambda x:x.split(),\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")\n",
    "keysents = summarizer.summarize(sents, topk=3)\n",
    "for _, _, sent in keysents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그러다보니 신 교수님의 주장은 시중에 나도는 어처구니 없는 주장인 ‘경기도가 지역화폐로 깡을 하게 한다’느니 ‘중간 업자가 끼어서 부정 소지가 있다’느니 하는 것들과 다를바 없이 들렸습니다.\n",
      "경기도내 지역화폐는 지류(상품권)와 카드나 앱 등 전자화폐가 있고 어느 것을 사용할 지는 시군이 선택합니다만\n",
      "전자화폐역시 전산시스템 운영자에게 소액의 약정된 수수료를 지급하고 사용자는 단 한푼의 손실도 없이 전액을 체크카드처럼 사용하기 때문에 중간 사업자의 비리란 있을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "komoran = Komoran()\n",
    "def komoran_tokenizer(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words\n",
    "\n",
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = komoran_tokenizer,\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")\n",
    "keysents = summarizer.summarize(sents, topk=3)\n",
    "for _, _, sent in keysents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이것은',\n",
       " '부분단',\n",
       " '분단어',\n",
       " '단어의',\n",
       " '예시입',\n",
       " '시입니',\n",
       " '입니다',\n",
       " '짧은',\n",
       " '어절은',\n",
       " '그대로',\n",
       " '나옵니',\n",
       " '옵니다']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subword_tokenizer(sent, n=3):\n",
    "    def subword(token, n):\n",
    "        if len(token) <= n:\n",
    "            return [token]\n",
    "        return [token[i:i+n] for i in range(len(token) - n + 1)]\n",
    "    return [sub for token in sent.split() for sub in subword(token, n)]\n",
    "\n",
    "subword_tokenizer('이것은 부분단어의 예시입니다 짧은 어절은 그대로 나옵니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그러다보니 신 교수님의 주장은 시중에 나도는 어처구니 없는 주장인 ‘경기도가 지역화폐로 깡을 하게 한다’느니 ‘중간 업자가 끼어서 부정 소지가 있다’느니 하는 것들과 다를바 없이 들렸습니다.\n",
      "신세돈 교수님, 지역화폐 비판 주장의 논거를 마저 듣고 싶습니다\n",
      "지역화폐는 이미 5년 이상 성남시와 경기도에서는 물론 전국으로 확산되어 아무 부작용이나 비리 없이 투명하게 잘 사용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = subword_tokenizer,\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")\n",
    "keysents = summarizer.summarize(sents, topk=3)\n",
    "for _, _, sent in keysents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근혜 토론에서의 ‘비리 연루 가능성’ 주장이 잘못임을 인정하시면 발언을 수정해 주시고, 여전히 동일한 주장을 하시면 그 논거를 마저 듣고 싶으니 공개토론에 한 번 더 응해주시면 좋겠습니다\n",
      "신세돈 교수님, 지역화폐 비판 주장의 논거를 마저 듣고 싶습니다\n",
      "그러다보니 신 교수님의 주장은 시중에 나도는 어처구니 없는 주장인 ‘경기도가 지역화폐로 깡을 하게 한다’느니 ‘중간 업자가 끼어서 부정 소지가 있다’느니 하는 것들과 다를바 없이 들렸습니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bias = np.ones(len(sents))\n",
    "bias[-1] = 10\n",
    "\n",
    "keysents = summarizer.summarize(sents, topk=3, bias=bias)\n",
    "for _, _, sent in keysents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
